{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "from skimage.transform import resize as imresize\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VgSceneGraphDataset(Dataset):\n",
    "    def __init__(self, vocab, h5_path, image_dir, image_size=(256, 256),\n",
    "                 normalize_images=True, max_objects=10, max_samples=None,\n",
    "                 include_relationships=True, use_orphaned_objects=True,\n",
    "                 left_right_flip=False):\n",
    "        super(VgSceneGraphDataset, self).__init__()\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.image_size = image_size\n",
    "        self.vocab = vocab\n",
    "        self.num_objects = len(vocab['object_idx_to_name'])\n",
    "        self.use_orphaned_objects = use_orphaned_objects\n",
    "        self.max_objects = max_objects\n",
    "        self.max_samples = max_samples\n",
    "        self.left_right_flip = left_right_flip\n",
    "        self.include_relationships = include_relationships\n",
    "        transform = [Resize(image_size), T.ToTensor()]\n",
    "        if normalize_images:\n",
    "            transform.append(imagenet_preprocess())\n",
    "        self.transform = T.Compose(transform)\n",
    "\n",
    "        self.data = {}\n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            for k, v in f.items():\n",
    "                if k == 'image_paths':\n",
    "                    self.image_paths = list(v)\n",
    "                else:\n",
    "                    self.data[k] = torch.IntTensor(np.asarray(v))\n",
    "\n",
    "    def __len__(self):\n",
    "        num = self.data['object_names'].size(0)\n",
    "        if self.max_samples is not None:\n",
    "            return min(self.max_samples, num)\n",
    "        if self.left_right_flip:\n",
    "            return num * 2\n",
    "        return num\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple of:\n",
    "        - image: FloatTensor of shape (C, H, W)\n",
    "        - objs: LongTensor of shape (O,)\n",
    "        - boxes: FloatTensor of shape (O, 4) giving boxes for objects in\n",
    "          (x0, y0, x1, y1) format, in a [0, 1] coordinate system.\n",
    "        - triples: LongTensor of shape (T, 3) where triples[t] = [i, p, j]\n",
    "          means that (objs[i], p, objs[j]) is a triple.\n",
    "        \"\"\"\n",
    "        flip = False\n",
    "        if index >= self.data['object_names'].size(0):\n",
    "            index = index - self.data['object_names'].size(0)\n",
    "            flip = True\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, self.image_paths[index])\n",
    "\n",
    "        with open(img_path, 'rb') as f:\n",
    "            with PIL.Image.open(f) as image:\n",
    "                if flip:\n",
    "                    image = PIL.ImageOps.mirror(image)\n",
    "                WW, HH = image.size\n",
    "                image = self.transform(image.convert('RGB'))\n",
    "\n",
    "        H, W = self.image_size\n",
    "\n",
    "        # Figure out which objects appear in relationships and which don't\n",
    "        obj_idxs_with_rels = set()\n",
    "        obj_idxs_without_rels = set(range(self.data['objects_per_image'][index].item()))\n",
    "        for r_idx in range(self.data['relationships_per_image'][index]):\n",
    "            s = self.data['relationship_subjects'][index, r_idx].item()\n",
    "            o = self.data['relationship_objects'][index, r_idx].item()\n",
    "            obj_idxs_with_rels.add(s)\n",
    "            obj_idxs_with_rels.add(o)\n",
    "            obj_idxs_without_rels.discard(s)\n",
    "            obj_idxs_without_rels.discard(o)\n",
    "\n",
    "        obj_idxs = list(obj_idxs_with_rels)\n",
    "        obj_idxs_without_rels = list(obj_idxs_without_rels)\n",
    "        if len(obj_idxs) > self.max_objects - 1:\n",
    "            obj_idxs = random.sample(obj_idxs, self.max_objects)\n",
    "        if len(obj_idxs) < self.max_objects - 1 and self.use_orphaned_objects:\n",
    "            num_to_add = self.max_objects - 1 - len(obj_idxs)\n",
    "            num_to_add = min(num_to_add, len(obj_idxs_without_rels))\n",
    "            obj_idxs += random.sample(obj_idxs_without_rels, num_to_add)\n",
    "        O = len(obj_idxs) + 1\n",
    "\n",
    "        objs = torch.LongTensor(self.max_objects+1).fill_(-1)\n",
    "\n",
    "        attrs = torch.FloatTensor(self.max_objects + 1, 80).fill_(0)\n",
    "        obj_index_offset = self.data['objects_per_image'][0:index].sum().item()\n",
    "\n",
    "        boxes = torch.FloatTensor([[0, 0, 1, 1]]).repeat(self.max_objects+1, 1)\n",
    "        obj_idx_mapping = {}\n",
    "        cnt = 0\n",
    "        for i, obj_idx in enumerate(obj_idxs):\n",
    "            objs[cnt] = self.data['object_names'][index, obj_idx].item()\n",
    "\n",
    "            for e in self.data['object_attributes'][obj_index_offset + obj_idx]:\n",
    "                if e.item() != -1:\n",
    "                    attrs[cnt][e] = 1\n",
    "\n",
    "            x, y, w, h = self.data['object_boxes'][index, obj_idx].tolist()\n",
    "            x0 = np.clip(float(x) / WW, 0, 1)\n",
    "            y0 = np.clip(float(y) / HH, 0, 1)\n",
    "            x1 = np.clip(float(w) / WW, 0, 1)\n",
    "            y1 = np.clip(float(h) / HH, 0, 1)\n",
    "            if x0+x1 > 1:\n",
    "                x1 = np.clip(x1, 0, 1 - x0)\n",
    "                if x1 < 0.02:\n",
    "                    print(\"x1<0.02\", x0, y0, x1, y1)\n",
    "                    continue\n",
    "            if y0+y1 > 1:\n",
    "                y1 = np.clip(y1, 0, 1 - y0)\n",
    "                if y1 < 0.02:\n",
    "                    print(\"y1<0.02\", x0, y0, x1, y1)\n",
    "                    continue\n",
    "            if flip:\n",
    "                x0 = 1 - (x0 + x1)\n",
    "            \n",
    "            boxes[cnt] = torch.FloatTensor([x0, y0, x1, y1])\n",
    "            obj_idx_mapping[obj_idx] = cnt\n",
    "            cnt += 1\n",
    "        O = cnt + 1\n",
    "        # The last object will be the special __image__ object\n",
    "        objs[O - 1] = self.vocab['object_name_to_idx']['__image__']\n",
    "\n",
    "        for i in range(O, self.max_objects+1):\n",
    "            # objs.append(self.vocab['object_name_to_idx']['__image__'])\n",
    "            # boxes.append(np.array([-0.6, -0.6, 0.5, 0.5]))\n",
    "            objs[i] = self.vocab['object_name_to_idx']['__image__']\n",
    "            boxes[i] = torch.FloatTensor([-0.6, -0.6, 0.5, 0.5])\n",
    "            attrs[i] = torch.FloatTensor(80).fill_(-1)\n",
    "\n",
    "        # triples = []\n",
    "        # for r_idx in range(self.data['relationships_per_image'][index].item()):\n",
    "        #     if not self.include_relationships:\n",
    "        #         break\n",
    "        #     s = self.data['relationship_subjects'][index, r_idx].item()\n",
    "        #     p = self.data['relationship_predicates'][index, r_idx].item()\n",
    "        #     o = self.data['relationship_objects'][index, r_idx].item()\n",
    "        #     s = obj_idx_mapping.get(s, None)\n",
    "        #     o = obj_idx_mapping.get(o, None)\n",
    "        #     if s is not None and o is not None:\n",
    "        #         triples.append([s, p, o])\n",
    "        #\n",
    "        # # Add dummy __in_image__ relationships for all objects\n",
    "        # in_image = self.vocab['pred_name_to_idx']['__in_image__']\n",
    "        # for i in range(O - 1):\n",
    "        #     triples.append([i, in_image, O - 1])\n",
    "        #\n",
    "        # triples = torch.LongTensor(triples)\n",
    "        return image, objs, boxes, attrs #, triples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    def __init__(self, size, interp=PIL.Image.BILINEAR):\n",
    "        if isinstance(size, tuple):\n",
    "              H, W = size\n",
    "              self.size = (W, H)\n",
    "        else:\n",
    "            self.size = (size, size)\n",
    "        self.interp = interp\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return img.resize(self.size, self.interp)\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.5, 0.5, 0.5]\n",
    "IMAGENET_STD = [0.5, 0.5, 0.5]\n",
    "\n",
    "INV_IMAGENET_MEAN = [-m for m in IMAGENET_MEAN]\n",
    "INV_IMAGENET_STD = [1.0 / s for s in IMAGENET_STD]\n",
    "\n",
    "\n",
    "def imagenet_preprocess():\n",
    "    return T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "\n",
    "def vg_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to be used when wrapping a VgSceneGraphDataset in a\n",
    "    DataLoader. Returns a tuple of the following:\n",
    "    \n",
    "    - imgs: FloatTensor of shape (N, C, H, W)\n",
    "    - objs: LongTensor of shape (O,) giving categories for all objects\n",
    "    - boxes: FloatTensor of shape (O, 4) giving boxes for all objects\n",
    "    - triples: FloatTensor of shape (T, 3) giving all triples, where\n",
    "    triples[t] = [i, p, j] means that [objs[i], p, objs[j]] is a triple\n",
    "    - obj_to_img: LongTensor of shape (O,) mapping objects to images;\n",
    "    obj_to_img[i] = n means that objs[i] belongs to imgs[n]\n",
    "    - triple_to_img: LongTensor of shape (T,) mapping triples to images;\n",
    "    triple_to_img[t] = n means that triples[t] belongs to imgs[n].\n",
    "    \"\"\"\n",
    "    # batch is a list, and each element is (image, objs, boxes, triples)\n",
    "    all_imgs, all_objs, all_boxes, all_triples = [], [], [], []\n",
    "    all_obj_to_img, all_triple_to_img = [], []\n",
    "    obj_offset = 0\n",
    "    for i, (img, objs, boxes, triples) in enumerate(batch):\n",
    "        all_imgs.append(img[None])\n",
    "        O, T = objs.size(0), triples.size(0)\n",
    "        all_objs.append(objs)\n",
    "        all_boxes.append(boxes)\n",
    "        triples = triples.clone()\n",
    "        triples[:, 0] += obj_offset\n",
    "        triples[:, 2] += obj_offset\n",
    "        all_triples.append(triples)\n",
    "\n",
    "        all_obj_to_img.append(torch.LongTensor(O).fill_(i))\n",
    "        all_triple_to_img.append(torch.LongTensor(T).fill_(i))\n",
    "        obj_offset += O\n",
    "\n",
    "    all_imgs = torch.cat(all_imgs)\n",
    "    all_objs = torch.cat(all_objs)\n",
    "    all_boxes = torch.cat(all_boxes)\n",
    "    all_triples = torch.cat(all_triples)\n",
    "    all_obj_to_img = torch.cat(all_obj_to_img)\n",
    "    all_triple_to_img = torch.cat(all_triple_to_img)\n",
    "\n",
    "    out = (all_imgs, all_objs, all_boxes, all_triples,\n",
    "           all_obj_to_img, all_triple_to_img)\n",
    "    return out\n",
    "\n",
    "\n",
    "def vg_uncollate_fn(batch):\n",
    "    \"\"\"\n",
    "    Inverse operation to the above.\n",
    "    \"\"\"\n",
    "    imgs, objs, boxes, triples, obj_to_img, triple_to_img = batch\n",
    "    out = []\n",
    "    obj_offset = 0\n",
    "    for i in range(imgs.size(0)):\n",
    "        cur_img = imgs[i]\n",
    "        o_idxs = (obj_to_img == i).nonzero().view(-1)\n",
    "        t_idxs = (triple_to_img == i).nonzero().view(-1)\n",
    "        cur_objs = objs[o_idxs]\n",
    "        cur_boxes = boxes[o_idxs]\n",
    "        cur_triples = triples[t_idxs].clone()\n",
    "        cur_triples[:, 0] -= obj_offset\n",
    "        cur_triples[:, 2] -= obj_offset\n",
    "        obj_offset += cur_objs.size(0)\n",
    "        out.append((cur_img, cur_objs, cur_boxes, cur_triples))\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
