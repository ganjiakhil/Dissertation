{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, json, os\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "from imageio import imread\n",
    "from cv2 import resize as imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VG_DIR = '/home/akhilkumar/Desktop/AttrLostGAN/datasets/vg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--output_h5_dir'], dest='output_h5_dir', nargs=None, const=None, default='/home/akhilkumar/Desktop/AttrLostGAN/datasets/vg', type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Input data\n",
    "parser.add_argument('--splits_json', default='./data/vg_splits.json')\n",
    "parser.add_argument('--images_json',\n",
    "    default=os.path.join(VG_DIR, 'image_data.json'))\n",
    "parser.add_argument('--objects_json',\n",
    "    default=os.path.join(VG_DIR, 'objects.json'))\n",
    "parser.add_argument('--attributes_json',\n",
    "    default=os.path.join(VG_DIR, 'attributes.json'))\n",
    "parser.add_argument('--object_aliases',\n",
    "    default=os.path.join(VG_DIR, 'object_alias.txt'))\n",
    "parser.add_argument('--relationship_aliases',\n",
    "    default=os.path.join(VG_DIR, 'relationship_alias.txt'))\n",
    "parser.add_argument('--relationships_json',\n",
    "    default=os.path.join(VG_DIR, 'relationships.json'))\n",
    "\n",
    "# Arguments for images\n",
    "parser.add_argument('--min_image_size', default=200, type=int)\n",
    "parser.add_argument('--train_split', default='train')\n",
    "\n",
    "# Arguments for objects\n",
    "parser.add_argument('--min_object_instances', default=2000, type=int)\n",
    "parser.add_argument('--min_attribute_instances', default=2000, type=int)\n",
    "parser.add_argument('--min_object_size', default=32, type=int)\n",
    "parser.add_argument('--min_objects_per_image', default=3, type=int)\n",
    "parser.add_argument('--max_objects_per_image', default=30, type=int)\n",
    "parser.add_argument('--max_attributes_per_image', default=30, type=int)\n",
    "\n",
    "# Arguments for relationships\n",
    "parser.add_argument('--min_relationship_instances', default=500, type=int)\n",
    "parser.add_argument('--min_relationships_per_image', default=1, type=int)\n",
    "parser.add_argument('--max_relationships_per_image', default=30, type=int)\n",
    "# Output\n",
    "parser.add_argument('--output_vocab_json', default=os.path.join(VG_DIR, 'vocab.json'))\n",
    "parser.add_argument('--output_h5_dir', default=VG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "  print('Loading image info from \"%s\"' % args.images_json)\n",
    "  with open(args.images_json, 'r') as f:\n",
    "    images = json.load(f)\n",
    "  image_id_to_image = {i['image_id']: i for i in images}\n",
    "\n",
    "  with open(args.splits_json, 'r') as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "  # Filter images for being too small\n",
    "  splits = remove_small_images(args, image_id_to_image, splits)\n",
    "\n",
    "  obj_aliases = load_aliases(args.object_aliases)\n",
    "  rel_aliases = load_aliases(args.relationship_aliases)\n",
    "\n",
    "  print('Loading objects from \"%s\"' % args.objects_json)\n",
    "  with open(args.objects_json, 'r') as f:\n",
    "    objects = json.load(f)\n",
    "\n",
    "  # Vocab for objects and relationships\n",
    "  vocab = {}\n",
    "  train_ids = splits[args.train_split]\n",
    "  create_object_vocab(args, train_ids, objects, obj_aliases, vocab)\n",
    "\n",
    "  print('Loading attributes from \"%s\"' % args.attributes_json)\n",
    "  with open(args.attributes_json, 'r') as f:\n",
    "    attributes = json.load(f)\n",
    "\n",
    "  # Vocab for attributes\n",
    "  create_attribute_vocab(args, train_ids, attributes, vocab)\n",
    "\n",
    "  object_id_to_obj = filter_objects(args, objects, obj_aliases, vocab, splits)\n",
    "  print('After filtering there are %d object instances'\n",
    "        % len(object_id_to_obj))\n",
    "\n",
    "  print('Loading relationshps from \"%s\"' % args.relationships_json)\n",
    "  with open(args.relationships_json, 'r') as f:\n",
    "    relationships = json.load(f)\n",
    "\n",
    "  create_rel_vocab(args, train_ids, relationships, object_id_to_obj,\n",
    "                   rel_aliases, vocab)\n",
    "\n",
    "  print('Encoding objects and relationships ...')\n",
    "  numpy_arrays = encode_graphs(args, splits, objects, relationships, vocab,\n",
    "                               object_id_to_obj, attributes)\n",
    "\n",
    "  print('Writing HDF5 output files')\n",
    "  for split_name, split_arrays in numpy_arrays.items():\n",
    "    image_ids = list(split_arrays['image_ids'].astype(int))\n",
    "    h5_path = os.path.join(args.output_h5_dir, '%s.h5' % split_name)\n",
    "    print('Writing file \"%s\"' % h5_path)\n",
    "    with h5py.File(h5_path, 'w') as h5_file:\n",
    "      for name, ary in split_arrays.items():\n",
    "        print('Creating datset: ', name, ary.shape, ary.dtype)\n",
    "        h5_file.create_dataset(name, data=ary)\n",
    "      print('Writing image paths')\n",
    "      image_paths = get_image_paths(image_id_to_image, image_ids)\n",
    "      path_dtype = h5py.special_dtype(vlen=str)\n",
    "      path_shape = (len(image_paths),)\n",
    "      path_dset = h5_file.create_dataset('image_paths', path_shape,\n",
    "                                         dtype=path_dtype)\n",
    "      for i, p in enumerate(image_paths):\n",
    "        path_dset[i] = p\n",
    "    print()\n",
    "\n",
    "  print('Writing vocab to \"%s\"' % args.output_vocab_json)\n",
    "  with open(args.output_vocab_json, 'w') as f:\n",
    "    json.dump(vocab, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_small_images(args, image_id_to_image, splits):\n",
    "  new_splits = {}\n",
    "  for split_name, image_ids in splits.items():\n",
    "    new_image_ids = []\n",
    "    num_skipped = 0\n",
    "    for image_id in image_ids:\n",
    "      image = image_id_to_image[image_id]\n",
    "      height, width = image['height'], image['width']\n",
    "      if min(height, width) < args.min_image_size:\n",
    "        num_skipped += 1\n",
    "        continue\n",
    "      new_image_ids.append(image_id)\n",
    "    new_splits[split_name] = new_image_ids\n",
    "    print('Removed %d images from split \"%s\" for being too small' %\n",
    "          (num_skipped, split_name))\n",
    "\n",
    "  return new_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(image_id_to_image, image_ids):\n",
    "  paths = []\n",
    "  for image_id in image_ids:\n",
    "    image = image_id_to_image[image_id]\n",
    "    base, filename = os.path.split(image['url'])\n",
    "    path = os.path.join(os.path.basename(base), filename)\n",
    "    paths.append(path)\n",
    "  return paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_images(args, image_ids, h5_file):\n",
    "  with open(args.images_json, 'r') as f:\n",
    "    images = json.load(f)\n",
    "  if image_ids:\n",
    "    image_ids = set(image_ids)\n",
    "\n",
    "  image_heights, image_widths = [], []\n",
    "  image_ids_out, image_paths = [], []\n",
    "  for image in images:\n",
    "    image_id = image['image_id']\n",
    "    if image_ids and image_id not in image_ids:\n",
    "      continue\n",
    "    height, width = image['height'], image['width']\n",
    "\n",
    "    base, filename = os.path.split(image['url'])\n",
    "    path = os.path.join(os.path.basename(base), filename)\n",
    "    image_paths.append(path)\n",
    "    image_heights.append(height)\n",
    "    image_widths.append(width)\n",
    "    image_ids_out.append(image_id)\n",
    "\n",
    "  image_ids_np = np.asarray(image_ids_out, dtype=int)\n",
    "  h5_file.create_dataset('image_ids', data=image_ids_np)\n",
    "\n",
    "  image_heights = np.asarray(image_heights, dtype=int)\n",
    "  h5_file.create_dataset('image_heights', data=image_heights)\n",
    "\n",
    "  image_widths = np.asarray(image_widths, dtype=int)\n",
    "  h5_file.create_dataset('image_widths', data=image_widths)\n",
    "\n",
    "  return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aliases(alias_path):\n",
    "  aliases = {}\n",
    "  print('Loading aliases from \"%s\"' % alias_path)\n",
    "  with open(alias_path, 'r') as f:\n",
    "    for line in f:\n",
    "      line = [s.strip() for s in line.split(',')]\n",
    "      for s in line:\n",
    "        aliases[s] = line[0]\n",
    "  return aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_object_vocab(args, image_ids, objects, aliases, vocab):\n",
    "  image_ids = set(image_ids)\n",
    "\n",
    "  print('Making object vocab from %d training images' % len(image_ids))\n",
    "  object_name_counter = Counter()\n",
    "  for image in objects:\n",
    "    if image['image_id'] not in image_ids:\n",
    "      continue\n",
    "    for obj in image['objects']:\n",
    "      names = set()\n",
    "      for name in obj['names']:\n",
    "        names.add(aliases.get(name, name))\n",
    "      object_name_counter.update(names)\n",
    "\n",
    "  object_names = ['__image__']\n",
    "  for name, count in object_name_counter.most_common():\n",
    "    if count >= args.min_object_instances:\n",
    "      object_names.append(name)\n",
    "  print('Found %d object categories with >= %d training instances' %\n",
    "        (len(object_names), args.min_object_instances))\n",
    "\n",
    "  object_name_to_idx = {}\n",
    "  object_idx_to_name = []\n",
    "  for idx, name in enumerate(object_names):\n",
    "    object_name_to_idx[name] = idx\n",
    "    object_idx_to_name.append(name)\n",
    "\n",
    "  vocab['object_name_to_idx'] = object_name_to_idx\n",
    "  vocab['object_idx_to_name'] = object_idx_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attribute_vocab(args, image_ids, attributes, vocab):\n",
    "  image_ids = set(image_ids)\n",
    "  print('Making attribute vocab from %d training images' % len(image_ids))\n",
    "  attribute_name_counter = Counter()\n",
    "  for image in attributes:\n",
    "    if image['image_id'] not in image_ids:\n",
    "      continue\n",
    "    for attribute in image['attributes']:\n",
    "      names = set()\n",
    "      try:\n",
    "        for name in attribute['attributes']:\n",
    "          names.add(name)\n",
    "        attribute_name_counter.update(names)\n",
    "      except KeyError:\n",
    "        pass\n",
    "  attribute_names = []\n",
    "  for name, count in attribute_name_counter.most_common():\n",
    "    if count >= args.min_attribute_instances:\n",
    "      attribute_names.append(name)\n",
    "  print('Found %d attribute categories with >= %d training instances' %\n",
    "        (len(attribute_names), args.min_attribute_instances))\n",
    "\n",
    "  attribute_name_to_idx = {}\n",
    "  attribute_idx_to_name = []\n",
    "  for idx, name in enumerate(attribute_names):\n",
    "    attribute_name_to_idx[name] = idx\n",
    "    attribute_idx_to_name.append(name)\n",
    "  vocab['attribute_name_to_idx'] = attribute_name_to_idx\n",
    "  vocab['attribute_idx_to_name'] = attribute_idx_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_objects(args, objects, aliases, vocab, splits):\n",
    "  object_id_to_objects = {}\n",
    "  all_image_ids = set()\n",
    "  for image_ids in splits.values():\n",
    "    all_image_ids |= set(image_ids)\n",
    "\n",
    "  object_name_to_idx = vocab['object_name_to_idx']\n",
    "  object_id_to_obj = {}\n",
    "\n",
    "  num_too_small = 0\n",
    "  for image in objects:\n",
    "    image_id = image['image_id']\n",
    "    if image_id not in all_image_ids:\n",
    "      continue\n",
    "    for obj in image['objects']:\n",
    "      object_id = obj['object_id']\n",
    "      final_name = None\n",
    "      final_name_idx = None\n",
    "      for name in obj['names']:\n",
    "        name = aliases.get(name, name)\n",
    "        if name in object_name_to_idx:\n",
    "          final_name = name\n",
    "          final_name_idx = object_name_to_idx[final_name]\n",
    "          break\n",
    "      w, h = obj['w'], obj['h']\n",
    "      too_small = (w < args.min_object_size) or (h < args.min_object_size)\n",
    "      if too_small:\n",
    "        num_too_small += 1\n",
    "      if final_name is not None and not too_small:\n",
    "        object_id_to_obj[object_id] = {\n",
    "          'name': final_name,\n",
    "          'name_idx': final_name_idx,\n",
    "          'box': [obj['x'], obj['y'], obj['w'], obj['h']],\n",
    "        }\n",
    "  print('Skipped %d objects with size < %d' % (num_too_small, args.min_object_size))\n",
    "  return object_id_to_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_graphs(args, splits, objects, relationships, vocab,\n",
    "                  object_id_to_obj, attributes):\n",
    "\n",
    "  image_id_to_objects = {}\n",
    "  for image in objects:\n",
    "    image_id = image['image_id']\n",
    "    image_id_to_objects[image_id] = image['objects']\n",
    "  image_id_to_relationships = {}\n",
    "  for image in relationships:\n",
    "    image_id = image['image_id']\n",
    "    image_id_to_relationships[image_id] = image['relationships']\n",
    "  image_id_to_attributes = {}\n",
    "  for image in attributes:\n",
    "    image_id = image['image_id']\n",
    "    image_id_to_attributes[image_id] = image['attributes']\n",
    "\n",
    "  numpy_arrays = {}\n",
    "  for split, image_ids in splits.items():\n",
    "    skip_stats = defaultdict(int)\n",
    "    # We need to filter *again* based on number of objects and relationships\n",
    "    final_image_ids = []\n",
    "    object_ids = []\n",
    "    object_names = []\n",
    "    object_boxes = []\n",
    "    objects_per_image = []\n",
    "    relationship_ids = []\n",
    "    relationship_subjects = []\n",
    "    relationship_predicates = []\n",
    "    relationship_objects = []\n",
    "    relationships_per_image = []\n",
    "    attribute_ids = []\n",
    "    attributes_per_object = []\n",
    "    object_attributes = []\n",
    "    for image_id in image_ids:\n",
    "      image_object_ids = []\n",
    "      image_object_names = []\n",
    "      image_object_boxes = []\n",
    "      object_id_to_idx = {}\n",
    "      for obj in image_id_to_objects[image_id]:\n",
    "        object_id = obj['object_id']\n",
    "        if object_id not in object_id_to_obj:\n",
    "          continue\n",
    "        obj = object_id_to_obj[object_id]\n",
    "        object_id_to_idx[object_id] = len(image_object_ids)\n",
    "        image_object_ids.append(object_id)\n",
    "        image_object_names.append(obj['name_idx'])\n",
    "        image_object_boxes.append(obj['box'])\n",
    "      num_objects = len(image_object_ids)\n",
    "      too_few = num_objects < args.min_objects_per_image\n",
    "      too_many = num_objects > args.max_objects_per_image\n",
    "      if too_few:\n",
    "        skip_stats['too_few_objects'] += 1\n",
    "        continue\n",
    "      if too_many:\n",
    "        skip_stats['too_many_objects'] += 1\n",
    "        continue\n",
    "      image_rel_ids = []\n",
    "      image_rel_subs = []\n",
    "      image_rel_preds = []\n",
    "      image_rel_objs = []\n",
    "      for rel in image_id_to_relationships[image_id]:\n",
    "        relationship_id = rel['relationship_id']\n",
    "        pred = rel['predicate']\n",
    "        pred_idx = vocab['pred_name_to_idx'].get(pred, None)\n",
    "        if pred_idx is None:\n",
    "          continue\n",
    "        sid = rel['subject']['object_id']\n",
    "        sidx = object_id_to_idx.get(sid, None)\n",
    "        oid = rel['object']['object_id']\n",
    "        oidx = object_id_to_idx.get(oid, None)\n",
    "        if sidx is None or oidx is None:\n",
    "          continue\n",
    "        image_rel_ids.append(relationship_id)\n",
    "        image_rel_subs.append(sidx)\n",
    "        image_rel_preds.append(pred_idx)\n",
    "        image_rel_objs.append(oidx)\n",
    "      num_relationships = len(image_rel_ids)\n",
    "      too_few = num_relationships < args.min_relationships_per_image\n",
    "      too_many = num_relationships > args.max_relationships_per_image\n",
    "      if too_few:\n",
    "        skip_stats['too_few_relationships'] += 1\n",
    "        continue\n",
    "      if too_many:\n",
    "        skip_stats['too_many_relationships'] += 1\n",
    "        continue\n",
    "\n",
    "      obj_id_to_attributes = {}\n",
    "      num_attributes = []\n",
    "      for obj_attribute in image_id_to_attributes[image_id]:\n",
    "        obj_id_to_attributes[obj_attribute['object_id']] = obj_attribute.get('attributes', None)\n",
    "      for object_id in image_object_ids:\n",
    "        attributes = obj_id_to_attributes.get(object_id, None)\n",
    "        if attributes is None:\n",
    "          object_attributes.append([-1] * args.max_attributes_per_image)\n",
    "          num_attributes.append(0)\n",
    "        else:\n",
    "          attribute_ids = []\n",
    "          for attribute in attributes:\n",
    "            if attribute in vocab['attribute_name_to_idx']:\n",
    "              attribute_ids.append(vocab['attribute_name_to_idx'][attribute])\n",
    "            if len(attribute_ids) >= args.max_attributes_per_image:\n",
    "              break\n",
    "          num_attributes.append(len(attribute_ids))\n",
    "          pad_len = args.max_attributes_per_image - len(attribute_ids)\n",
    "          attribute_ids = attribute_ids + [-1] * pad_len\n",
    "          object_attributes.append(attribute_ids)\n",
    "\n",
    "      # Pad object info out to max_objects_per_image\n",
    "      while len(image_object_ids) < args.max_objects_per_image:\n",
    "        image_object_ids.append(-1)\n",
    "        image_object_names.append(-1)\n",
    "        image_object_boxes.append([-1, -1, -1, -1])\n",
    "        num_attributes.append(-1)\n",
    "\n",
    "      # Pad relationship info out to max_relationships_per_image\n",
    "      while len(image_rel_ids) < args.max_relationships_per_image:\n",
    "        image_rel_ids.append(-1)\n",
    "        image_rel_subs.append(-1)\n",
    "        image_rel_preds.append(-1)\n",
    "        image_rel_objs.append(-1)\n",
    "\n",
    "      final_image_ids.append(image_id)\n",
    "      object_ids.append(image_object_ids)\n",
    "      object_names.append(image_object_names)\n",
    "      object_boxes.append(image_object_boxes)\n",
    "      objects_per_image.append(num_objects)\n",
    "      relationship_ids.append(image_rel_ids)\n",
    "      relationship_subjects.append(image_rel_subs)\n",
    "      relationship_predicates.append(image_rel_preds)\n",
    "      relationship_objects.append(image_rel_objs)\n",
    "      relationships_per_image.append(num_relationships)\n",
    "      attributes_per_object.append(num_attributes)\n",
    "\n",
    "    print('Skip stats for split \"%s\"' % split)\n",
    "    for stat, count in skip_stats.items():\n",
    "      print(stat, count)\n",
    "    print()\n",
    "    numpy_arrays[split] = {\n",
    "      'image_ids': np.asarray(final_image_ids),\n",
    "      'object_ids': np.asarray(object_ids),\n",
    "      'object_names': np.asarray(object_names),\n",
    "      'object_boxes': np.asarray(object_boxes),\n",
    "      'objects_per_image': np.asarray(objects_per_image),\n",
    "      'relationship_ids': np.asarray(relationship_ids),\n",
    "      'relationship_subjects': np.asarray(relationship_subjects),\n",
    "      'relationship_predicates': np.asarray(relationship_predicates),\n",
    "      'relationship_objects': np.asarray(relationship_objects),\n",
    "      'relationships_per_image': np.asarray(relationships_per_image),\n",
    "      'attributes_per_object': np.asarray(attributes_per_object),\n",
    "      'object_attributes': np.asarray(object_attributes),\n",
    "    }\n",
    "    for k, v in numpy_arrays[split].items():\n",
    "      if v.dtype == np.int64:\n",
    "        numpy_arrays[split][k] = v.astype(np.int32)\n",
    "  return numpy_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rel_vocab(args, image_ids, relationships, object_id_to_obj,\n",
    "                     rel_aliases, vocab):\n",
    "  pred_counter = defaultdict(int)\n",
    "  image_ids_set = set(image_ids)\n",
    "  for image in relationships:\n",
    "    image_id = image['image_id']\n",
    "    if image_id not in image_ids_set:\n",
    "      continue\n",
    "    for rel in image['relationships']:\n",
    "      sid = rel['subject']['object_id']\n",
    "      oid = rel['object']['object_id']\n",
    "      found_subject = sid in object_id_to_obj\n",
    "      found_object = oid in object_id_to_obj\n",
    "      if not found_subject or not found_object:\n",
    "        continue\n",
    "      pred = rel['predicate'].lower().strip()\n",
    "      pred = rel_aliases.get(pred, pred)\n",
    "      rel['predicate'] = pred\n",
    "      pred_counter[pred] += 1\n",
    "\n",
    "  pred_names = ['__in_image__']\n",
    "  for pred, count in pred_counter.items():\n",
    "    if count >= args.min_relationship_instances:\n",
    "      pred_names.append(pred)\n",
    "  print('Found %d relationship types with >= %d training instances'\n",
    "        % (len(pred_names), args.min_relationship_instances))\n",
    "\n",
    "  pred_name_to_idx = {}\n",
    "  pred_idx_to_name = []\n",
    "  for idx, name in enumerate(pred_names):\n",
    "    pred_name_to_idx[name] = idx\n",
    "    pred_idx_to_name.append(name)\n",
    "\n",
    "  vocab['pred_name_to_idx'] = pred_name_to_idx\n",
    "  vocab['pred_idx_to_name'] = pred_idx_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  args = parser.parse_args()\n",
    "  main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('env_p1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "106aa6d17bafa1f5a49348e7858ed546d0bba7425cbcd244f6fa58b6a299668b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
